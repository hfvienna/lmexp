{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -e ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists('path/to/your/lmexp/models/implementations/llama3.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/lmexp/lmexp/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists('../llama3.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "    gpt2small.ipynb\n",
      "    .ipynb_checkpoints/\n",
      "        gpt2small-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "\n",
    "list_files('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 file exists: False\n",
      "Llama3 file exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"GPT2 file exists:\", os.path.exists('./models/implementations/gpt2small.py'))\n",
    "print(\"Llama3 file exists:\", os.path.exists('./models/implementations/llama3.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libcudnn.so.8: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt2small\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpt2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(gpt2\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lmexp-0.1-py3.10.egg/lmexp/models/implementations/gpt2small.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooked_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/utils/__init__.py:33\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     27\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     ContextManagers,\n\u001b[1;32m     35\u001b[0m     ExplicitEnum,\n\u001b[1;32m     36\u001b[0m     ModelOutput,\n\u001b[1;32m     37\u001b[0m     PaddingStrategy,\n\u001b[1;32m     38\u001b[0m     TensorType,\n\u001b[1;32m     39\u001b[0m     add_model_info_to_auto_map,\n\u001b[1;32m     40\u001b[0m     cached_property,\n\u001b[1;32m     41\u001b[0m     can_return_loss,\n\u001b[1;32m     42\u001b[0m     expand_dims,\n\u001b[1;32m     43\u001b[0m     find_labels,\n\u001b[1;32m     44\u001b[0m     flatten_dict,\n\u001b[1;32m     45\u001b[0m     infer_framework,\n\u001b[1;32m     46\u001b[0m     is_jax_tensor,\n\u001b[1;32m     47\u001b[0m     is_numpy_array,\n\u001b[1;32m     48\u001b[0m     is_tensor,\n\u001b[1;32m     49\u001b[0m     is_tf_symbolic_tensor,\n\u001b[1;32m     50\u001b[0m     is_tf_tensor,\n\u001b[1;32m     51\u001b[0m     is_torch_device,\n\u001b[1;32m     52\u001b[0m     is_torch_dtype,\n\u001b[1;32m     53\u001b[0m     is_torch_tensor,\n\u001b[1;32m     54\u001b[0m     reshape,\n\u001b[1;32m     55\u001b[0m     squeeze,\n\u001b[1;32m     56\u001b[0m     strtobool,\n\u001b[1;32m     57\u001b[0m     tensor_size,\n\u001b[1;32m     58\u001b[0m     to_numpy,\n\u001b[1;32m     59\u001b[0m     to_py_obj,\n\u001b[1;32m     60\u001b[0m     transpose,\n\u001b[1;32m     61\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     65\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[1;32m     95\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m     torch_only_method,\n\u001b[1;32m    211\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/utils/generic.py:461\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libcudnn.so.8: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import lmexp.models.implementations.gpt2small as gpt2\n",
    "print(gpt2.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages/accelerate-0.31.0-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/ipykernel-6.29.4-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/torch-2.3.1-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/requests-2.32.3-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/python_dotenv-1.0.1-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/numpy-2.0.0-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/bitsandbytes-0.42.0-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/anthropic-0.29.0-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/safetensors-0.4.3-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/huggingface_hub-0.23.4-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/tokenizers-0.19.1-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/regex-2024.5.15-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/triton-2.3.1-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_nvtx_cu12-12.1.105-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_nccl_cu12-2.20.5-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cusparse_cu12-12.1.0.106-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cusolver_cu12-11.4.5.107-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_curand_cu12-10.3.2.106-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cufft_cu12-11.0.2.54-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cublas_cu12-12.1.3.1-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cudnn_cu12-8.9.2.26-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cuda_cupti_cu12-12.1.105-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cuda_runtime_cu12-12.1.105-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_cuda_nvrtc_cu12-12.1.105-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/scipy-1.14.0-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/pydantic-2.8.2-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/jiter-0.5.0-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/nvidia_nvjitlink_cu12-12.5.82-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/pydantic_core-2.20.1-py3.10-linux-x86_64.egg', '/opt/conda/lib/python3.10/site-packages/annotated_types-0.7.0-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/jsonpointer-3.0.0-py3.10.egg', '/opt/conda/lib/python3.10/site-packages/lmexp-0.1-py3.10.egg', '/root/lmexp']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lmexp.models.implementations.gpt2small' from '/opt/conda/lib/python3.10/site-packages/lmexp-0.1-py3.10.egg/lmexp/models/implementations/gpt2small.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import lmexp.models.implementations.gpt2small as gpt2\n",
    "importlib.reload(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2492339924.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    echo $LD_LIBRARY_PATH\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libcudnn.so.8: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libcudnn.so.8: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libcudnn.so.8: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt2small\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(GPT2Tokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lmexp-0.1-py3.10.egg/lmexp/models/implementations/gpt2small.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooked_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/utils/__init__.py:33\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     27\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     ContextManagers,\n\u001b[1;32m     35\u001b[0m     ExplicitEnum,\n\u001b[1;32m     36\u001b[0m     ModelOutput,\n\u001b[1;32m     37\u001b[0m     PaddingStrategy,\n\u001b[1;32m     38\u001b[0m     TensorType,\n\u001b[1;32m     39\u001b[0m     add_model_info_to_auto_map,\n\u001b[1;32m     40\u001b[0m     cached_property,\n\u001b[1;32m     41\u001b[0m     can_return_loss,\n\u001b[1;32m     42\u001b[0m     expand_dims,\n\u001b[1;32m     43\u001b[0m     find_labels,\n\u001b[1;32m     44\u001b[0m     flatten_dict,\n\u001b[1;32m     45\u001b[0m     infer_framework,\n\u001b[1;32m     46\u001b[0m     is_jax_tensor,\n\u001b[1;32m     47\u001b[0m     is_numpy_array,\n\u001b[1;32m     48\u001b[0m     is_tensor,\n\u001b[1;32m     49\u001b[0m     is_tf_symbolic_tensor,\n\u001b[1;32m     50\u001b[0m     is_tf_tensor,\n\u001b[1;32m     51\u001b[0m     is_torch_device,\n\u001b[1;32m     52\u001b[0m     is_torch_dtype,\n\u001b[1;32m     53\u001b[0m     is_torch_tensor,\n\u001b[1;32m     54\u001b[0m     reshape,\n\u001b[1;32m     55\u001b[0m     squeeze,\n\u001b[1;32m     56\u001b[0m     strtobool,\n\u001b[1;32m     57\u001b[0m     tensor_size,\n\u001b[1;32m     58\u001b[0m     to_numpy,\n\u001b[1;32m     59\u001b[0m     to_py_obj,\n\u001b[1;32m     60\u001b[0m     transpose,\n\u001b[1;32m     61\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     65\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[1;32m     95\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m     torch_only_method,\n\u001b[1;32m    211\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers-4.41.2-py3.10.egg/transformers/utils/generic.py:461\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libcudnn.so.8: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from lmexp.models.implementations.gpt2small import GPT2Tokenizer\n",
    "print(GPT2Tokenizer.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lmexp-0.1-py3.10.egg/lmexp/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import lmexp\n",
    "print(lmexp.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lmexp.models.implementations.llama3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt2small\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, ProbedGPT2\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplementations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama3Tokenizer, ProbedLlama3\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_probe\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcaa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_caa_vecs\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lmexp.models.implementations.llama3'"
     ]
    }
   ],
   "source": [
    "from lmexp.models.implementations.gpt2small import GPT2Tokenizer, ProbedGPT2\n",
    "from lmexp.models.implementations.llama3 import Llama3Tokenizer, ProbedLlama3\n",
    "from lmexp.generic.probing import train_probe\n",
    "from lmexp.generic.caa import get_caa_vecs\n",
    "from lmexp.generic.hooked_model import run_simple_steering\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GPT2 model and tokenizer\n",
    "\n",
    "These classes have already implemented all the probing-related methods so we won't have to add more hooks + they are ready to use with our vector extraction and steering functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c1ce2e8c254cecac1cc3c6d826cd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7891682167ea4fe8b989dce4025eb426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c009d305c34fdf87adc9920027b7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940cd727a7fe4819adfb014bcac392e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b70ea7c5c343cc9fc3a19fde2615d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e157eb85b8fb4c6aa466b45225d2ee6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a79fba0dc8c4838aff29733c790da2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ProbedGPT2()\n",
    "tokenizer = GPT2Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_n_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Llama 3 model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProbedLlama3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mProbedLlama3\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ProbedLlama3' is not defined"
     ]
    }
   ],
   "source": [
    "model = ProbedLlama3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Llama3Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a linear probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data\n",
    "\n",
    "Let's see whether we can get a date/time probe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labeled_text(n):\n",
    "    # date as text, date as utc timestamp in seconds, sample randomly from between 1990 and 2022\n",
    "    start_timestamp = datetime(2013, 1, 1).timestamp()\n",
    "    end_timestamp = datetime(2016, 1, 1).timestamp()\n",
    "    labeled_text = []\n",
    "    for i in range(n):\n",
    "        timestamp = start_timestamp + (end_timestamp - start_timestamp) * random.random()\n",
    "        date = datetime.fromtimestamp(timestamp)\n",
    "        # date like \"Monday 15th November 2021 8AM\"\n",
    "        text = date.strftime(\"Today is a %A. It's the %dth of %B, %Y. The time is %I %p. This is the point in time when\")\n",
    "        label = timestamp\n",
    "        labeled_text.append((text, label))\n",
    "    # normalize labels to have mean 0 and std 1\n",
    "    labels = [label for _, label in labeled_text]\n",
    "    mean = sum(labels) / len(labels)\n",
    "    std = (sum((label - mean) ** 2 for label in labels) / len(labels)) ** 0.5\n",
    "    labeled_text = [(text, (label - mean) / std) for text, label in labeled_text]\n",
    "    return labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Today is a Sunday. It's the 19th of May, 2013. The time is 09 PM. This is the point in time when\", -1.280076907780599)\n"
     ]
    }
   ],
   "source": [
    "data = gen_labeled_text(10_000)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:06<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss: 5.525280564117431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean loss: 0.9294690269470215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, mean loss: 0.8781939834594726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, mean loss: 0.7769097785949707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, mean loss: 0.7447526664733887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, mean loss: 0.6211295169830322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, mean loss: 0.5770946910858155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, mean loss: 0.49429606513977054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, mean loss: 0.4381583423614502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, mean loss: 0.3594137226104736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, mean loss: 0.3357479923248291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, mean loss: 0.2863012512207031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, mean loss: 0.26376518878936767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, mean loss: 0.2247169054031372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, mean loss: 0.21504546127319335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, mean loss: 0.17872962017059327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, mean loss: 0.20753318367004395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, mean loss: 0.1643917332649231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, mean loss: 0.15300347843170167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:05<00:00, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, mean loss: 0.13035983095169068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probe = train_probe(\n",
    "    labeled_text=data,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=4,\n",
    "    n_epochs=20,\n",
    "    batch_size=128,\n",
    "    lr=1e-2,\n",
    "    save_to=None,\n",
    "    token_position=-2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = probe.weight[0]\n",
    "bias = probe.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0284], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'The current date is',\n",
       "  'output': 'The current date is the date of the first occurrence',\n",
       "  'layer': 4,\n",
       "  'multiplier': -3}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_steering(\n",
    "    text=[\"The current date is\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=4,\n",
    "    multiplier=-3,\n",
    "    vector=direction.detach(),\n",
    "    max_n_tokens=10,\n",
    "    save_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'The current date is',\n",
       "  'output': 'The current date is the next year, the next',\n",
       "  'layer': 4,\n",
       "  'multiplier': 4}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_steering(\n",
    "    text=[\"The current date is\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=4,\n",
    "    multiplier=4,\n",
    "    vector=direction.detach(),\n",
    "    max_n_tokens=10,\n",
    "    save_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD = [\n",
    "    \"The weather is really nice\",\n",
    "    \"I'm so happy\",\n",
    "    \"This cake is absolutely delicious\",\n",
    "    \"I love my friends\",\n",
    "    \"I'm feeling great\",\n",
    "    \"I'm so excited\",\n",
    "    \"This is the best day ever\",\n",
    "    \"I really like this gift\",\n",
    "    \"Croissants are my favorite\",\n",
    "    \"The movie was fantastic\",\n",
    "    \"I got a promotion at work\",\n",
    "    \"My vacation was amazing\",\n",
    "    \"The concert exceeded my expectations\",\n",
    "    \"I'm grateful for my family\",\n",
    "    \"This book is incredibly engaging\",\n",
    "    \"The restaurant service was excellent\",\n",
    "    \"I'm proud of my accomplishments\",\n",
    "    \"The sunset is breathtakingly beautiful\",\n",
    "    \"I passed my exam with flying colors\",\n",
    "    \"This coffee tastes perfect\",\n",
    "]\n",
    "\n",
    "BAD = [\n",
    "    \"The weather is really bad\",\n",
    "    \"I'm so sad\",\n",
    "    \"This cake is completely inedible\",\n",
    "    \"I hate my enemies\",\n",
    "    \"I'm feeling awful\",\n",
    "    \"I'm so anxious\",\n",
    "    \"This is the worst day ever\",\n",
    "    \"I dislike this gift\",\n",
    "    \"Croissants are disgusting\",\n",
    "    \"The movie was terrible\",\n",
    "    \"I got fired from work\",\n",
    "    \"My vacation was a disaster\",\n",
    "    \"The concert was a huge disappointment\",\n",
    "    \"I'm frustrated with my family\",\n",
    "    \"This book is incredibly boring\",\n",
    "    \"The restaurant service was horrible\",\n",
    "    \"I'm ashamed of my mistakes\",\n",
    "    \"The weather is depressingly gloomy\",\n",
    "    \"I failed my exam miserably\",\n",
    "    \"This coffee tastes awful\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (text, True) for text in GOOD\n",
    "] + [\n",
    "    (text, False) for text in BAD\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 121.28it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(3, 8),\n",
    "    save_to=None              \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'I think that this cat is',\n",
       "  'output': \"I think that this cat is a bit of a mess. It's not a good cat. It\",\n",
       "  'layer': 6,\n",
       "  'multiplier': -2}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_steering(\n",
    "    text=[\"I think that this cat is\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=6,\n",
    "    multiplier=-2,\n",
    "    vector=vectors[6],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'I think that this cat is',\n",
       "  'output': 'I think that this cat is a great addition to the collection of cats that we have in our collection',\n",
       "  'layer': 6,\n",
       "  'multiplier': 2}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_simple_steering(\n",
    "    text=[\"I think that this cat is\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=6,\n",
    "    multiplier=2,\n",
    "    vector=vectors[6],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
