{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -e ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Successfully logged in to Hugging Face\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Hugging Face token from the environment variable\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Login using the token\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"HUGGINGFACE_TOKEN not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gpt2small import GPT2Tokenizer, ProbedGPT2\n",
    "from lmexp.models.implementations.llama2 import Llama2Tokenizer, ProbedLlama2\n",
    "from lmexp.models.implementations.llama3 import Llama3Tokenizer, ProbedLlama3\n",
    "from lmexp.generic.probing import train_probe\n",
    "from lmexp.generic.caa import get_caa_vecs\n",
    "from lmexp.generic.hooked_model import run_simple_steering\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load GPT2 model and tokenizer\n",
    "\n",
    "These classes have already implemented all the probing-related methods so we won't have to add more hooks + they are ready to use with our vector extraction and steering functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProbedGPT2()\n",
    "tokenizer = GPT2Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_n_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load Llama 2 model and tokenizer\n",
    "\n",
    "Don't run all models at the same time. Select one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProbedLlama2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Llama2Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_n_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Llama 3 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfa00b7cc024da7ae20ac13e30195a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d312207605402185885cc332725c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903f07c62c7f4d5bb05046fde2347eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c601bd3ad16149349a5e0ebc7058a764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236b611f56464a9c92210f6e3e4191b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b74f8b3ddcb44669e1220413fd05f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aa0f8ad1644d4992ca0df5ed3f67e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6e29e0f5da44258f0e8cf76e998dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ade9e728f24c3ca6da316b419d8733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ProbedLlama3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d8de3f0a2e4533876c0294032e4e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f328d9fbffc43ecb27638dfa3bb9017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf91f9af35b04a6e9dcdf81ba9fb8e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Llama3Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_n_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a linear probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data\n",
    "\n",
    "Let's see whether we can get a date/time probe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labeled_text(n):\n",
    "    # date as text, date as utc timestamp in seconds, sample randomly from between 1990 and 2022\n",
    "    start_timestamp = datetime(2013, 1, 1).timestamp()\n",
    "    end_timestamp = datetime(2016, 1, 1).timestamp()\n",
    "    labeled_text = []\n",
    "    for i in range(n):\n",
    "        timestamp = start_timestamp + (end_timestamp - start_timestamp) * random.random()\n",
    "        date = datetime.fromtimestamp(timestamp)\n",
    "        # date like \"Monday 15th November 2021 8AM\"\n",
    "        text = date.strftime(\"Today is a %A. It's the %dth of %B, %Y. The time is %I %p. This is the point in time when\")\n",
    "        label = timestamp\n",
    "        labeled_text.append((text, label))\n",
    "    # normalize labels to have mean 0 and std 1\n",
    "    labels = [label for _, label in labeled_text]\n",
    "    mean = sum(labels) / len(labels)\n",
    "    std = (sum((label - mean) ** 2 for label in labels) / len(labels)) ** 0.5\n",
    "    labeled_text = [(text, (label - mean) / std) for text, label in labeled_text]\n",
    "    return labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Today is a Thursday. It's the 03th of July, 2014. The time is 10 PM. This is the point in time when\", -0.0007876819166826553)\n",
      "Number of tokens: 33\n"
     ]
    }
   ],
   "source": [
    "data = gen_labeled_text(10_000)\n",
    "print(data[0])\n",
    "#Number of tokens\n",
    "encoded = tokenizer.encode(data[0][0])\n",
    "num_tokens = encoded.size(1)  # Get the size of the second dimension\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 128000 ()\n",
      "1: 15724 (Today)\n",
      "2: 374 ( is)\n",
      "3: 264 ( a)\n",
      "4: 7159 ( Monday)\n",
      "5: 13 (.)\n",
      "6: 1102 ( It)\n",
      "7: 596 ('s)\n",
      "8: 279 ( the)\n",
      "9: 220 ( )\n",
      "10: 2589 (07)\n",
      "11: 339 (th)\n",
      "12: 315 ( of)\n",
      "13: 6250 ( September)\n",
      "14: 11 (,)\n",
      "15: 220 ( )\n",
      "16: 679 (201)\n",
      "17: 20 (5)\n",
      "18: 13 (.)\n",
      "19: 578 ( The)\n",
      "20: 892 ( time)\n",
      "21: 374 ( is)\n",
      "22: 220 ( )\n",
      "23: 2371 (04)\n",
      "24: 6912 ( AM)\n",
      "25: 13 (.)\n",
      "26: 1115 ( This)\n",
      "27: 374 ( is)\n",
      "28: 279 ( the)\n",
      "29: 1486 ( point)\n",
      "30: 304 ( in)\n",
      "31: 892 ( time)\n",
      "32: 994 ( when)\n"
     ]
    }
   ],
   "source": [
    "#See what token(s) are are looking at:\n",
    "# Get the text from data[0]\n",
    "text = data[0][0]\n",
    "\n",
    "# Tokenize the text\n",
    "encoded = tokenizer.encode(text)\n",
    "\n",
    "# Get the token IDs (remove batch dimension and convert to list)\n",
    "token_ids = encoded.squeeze(0).tolist()\n",
    "\n",
    "# Print the tokens with their positions and IDs\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    token_text = tokenizer.decode(torch.tensor([token_id]))\n",
    "    print(f\"{i}: {token_id} ({token_text})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [04:34<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss: 1.8401461360931397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 147/156 [04:18<00:15,  1.75s/it]"
     ]
    }
   ],
   "source": [
    "layer = 16\n",
    "n_epochs = 5\n",
    "token_position = 1\n",
    "\n",
    "probe = train_probe(\n",
    "    labeled_text=data,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=layer,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=64, #Batch size 128 does not work for Llama 3 8GB at full precision with A100 80GB (64 requires 59.7GB)\n",
    "    lr=1e-2,\n",
    "    save_to=f\"probe_layer{layer}_epochs{n_epochs}_tokenpos{token_position}.pth\",\n",
    "    token_position=token_position\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = probe.weight[0]\n",
    "bias = probe.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boilerplate code:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simulate a model's residual dimension\n",
    "resid_dim = model.resid_dim()\n",
    "print(f\"Residual dimension: {resid_dim}\")\n",
    "\n",
    "# Inspect the probe\n",
    "print(\"Probe structure:\", probe)\n",
    "print(\"Probe weight shape:\", probe.weight.shape)\n",
    "print(\"Probe bias shape:\", probe.bias.shape)\n",
    "\n",
    "# Total number of parameters\n",
    "total_params = sum(p.numel() for p in probe.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "\n",
    "# Memory usage (assuming float32)\n",
    "memory_usage = total_params * 4 / 1024  # in KB\n",
    "print(f\"Approximate memory usage: {memory_usage:.2f} KB\")\n",
    "\n",
    "# Sample input\n",
    "sample_input = torch.randn(1, resid_dim)  # Batch size of 1\n",
    "\n",
    "# Forward pass\n",
    "#output = probe(sample_input)\n",
    "\n",
    "#print(\"Sample input shape:\", sample_input.shape)\n",
    "#print(\"Output shape:\", output.shape)\n",
    "#print(\"Output value:\", output.item())\n",
    "\n",
    "# Accessing weights and bias\n",
    "print(\"\\nFirst few weights:\")\n",
    "print(probe.weight[0, :10])  # First 10 weights\n",
    "print(\"\\nBias:\")\n",
    "print(probe.bias)\n",
    "\n",
    "# Demonstrating linearity\n",
    "scale = 2.0\n",
    "#scaled_output = probe(scale * sample_input)\n",
    "#print(f\"\\nOutput with input scaled by {scale}:\", scaled_output.item())\n",
    "#print(\"Original output scaled:\", scale * output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "probe_path = os.path.abspath(\"probe.pth\")\n",
    "print(f\"The probe should be at: {probe_path}\")\n",
    "print(f\"Does this file exist? {os.path.exists(probe_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "\n",
    "result = find_file('probe.pth', '/root')\n",
    "print(f\"File found at: {result}\" if result else \"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"Have write permission: {os.access(os.getcwd(), os.W_OK)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for multiplier in [-0.6, -0.5, -0.4, -0.3, 0, 0.3, 0.4, 0.5, 0.6]:\n",
    "    result = run_simple_steering(\n",
    "        text=[\"The YYYY of the current year is:\"],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        layer=16,\n",
    "        multiplier=multiplier,\n",
    "        vector=direction.detach(),\n",
    "        max_n_tokens=13,\n",
    "        save_to=None\n",
    "    )\n",
    "    if isinstance(result, list) and len(result) > 0:\n",
    "        if isinstance(result[0], dict) and 'output' in result[0]:\n",
    "            print(f\"Multiplier {multiplier}: {result[0]['output']}\")\n",
    "        else:\n",
    "            print(f\"Multiplier {multiplier}: {result}\")\n",
    "    else:\n",
    "        print(f\"Multiplier {multiplier}: Unexpected result format - {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simple_steering(\n",
    "    text=[\"The current date is\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=16,\n",
    "    multiplier=0,\n",
    "    vector=direction.detach(),\n",
    "    max_n_tokens=10,\n",
    "    save_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simple_steering(\n",
    "    text=[\"The current date is\"],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=16,\n",
    "    multiplier=0,\n",
    "    vector=direction.detach(),\n",
    "    max_n_tokens=10,\n",
    "    save_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD = [\n",
    "    \"The weather is really nice\",\n",
    "    \"I'm so happy\",\n",
    "    \"This cake is absolutely delicious\",\n",
    "    \"I love my friends\",\n",
    "    \"I'm feeling great\",\n",
    "    \"I'm so excited\",\n",
    "    \"This is the best day ever\",\n",
    "    \"I really like this gift\",\n",
    "    \"Croissants are my favorite\",\n",
    "    \"The movie was fantastic\",\n",
    "    \"I got a promotion at work\",\n",
    "    \"My vacation was amazing\",\n",
    "    \"The concert exceeded my expectations\",\n",
    "    \"I'm grateful for my family\",\n",
    "    \"This book is incredibly engaging\",\n",
    "    \"The restaurant service was excellent\",\n",
    "    \"I'm proud of my accomplishments\",\n",
    "    \"The sunset is breathtakingly beautiful\",\n",
    "    \"I passed my exam with flying colors\",\n",
    "    \"This coffee tastes perfect\",\n",
    "]\n",
    "\n",
    "BAD = [\n",
    "    \"The weather is really bad\",\n",
    "    \"I'm so sad\",\n",
    "    \"This cake is completely inedible\",\n",
    "    \"I hate my enemies\",\n",
    "    \"I'm feeling awful\",\n",
    "    \"I'm so anxious\",\n",
    "    \"This is the worst day ever\",\n",
    "    \"I dislike this gift\",\n",
    "    \"Croissants are disgusting\",\n",
    "    \"The movie was terrible\",\n",
    "    \"I got fired from work\",\n",
    "    \"My vacation was a disaster\",\n",
    "    \"The concert was a huge disappointment\",\n",
    "    \"I'm frustrated with my family\",\n",
    "    \"This book is incredibly boring\",\n",
    "    \"The restaurant service was horrible\",\n",
    "    \"I'm ashamed of my mistakes\",\n",
    "    \"The weather is depressingly gloomy\",\n",
    "    \"I failed my exam miserably\",\n",
    "    \"This coffee tastes awful\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BADDATE = [\n",
    "    \"The YYYY of the current year is: 1903\",\n",
    "    \"The YYYY of the current year is: 1918\",\n",
    "    \"The YYYY of the current year is: 1935\",\n",
    "    \"The YYYY of the current year is: 1941\",\n",
    "    \"The YYYY of the current year is: 1956\",\n",
    "    \"The YYYY of the current year is: 1962\",\n",
    "    \"The YYYY of the current year is: 1977\",\n",
    "    \"The YYYY of the current year is: 1989\",\n",
    "    \"The YYYY of the current year is: 1995\",\n",
    "    \"The YYYY of the current year is: 2008\",\n",
    "    \"The YYYY of the current year is: 1900\",\n",
    "    \"The YYYY of the current year is: 1922\",\n",
    "    \"The YYYY of the current year is: 1937\",\n",
    "    \"The YYYY of the current year is: 1949\",\n",
    "    \"The YYYY of the current year is: 1965\",\n",
    "    \"The YYYY of the current year is: 1978\",\n",
    "    \"The YYYY of the current year is: 1984\",\n",
    "    \"The YYYY of the current year is: 1997\",\n",
    "    \"The YYYY of the current year is: 2003\",\n",
    "    \"The YYYY of the current year is: 2011\",\n",
    "    \"The YYYY of the current year is: 1907\",\n",
    "    \"The YYYY of the current year is: 1926\",\n",
    "    \"The YYYY of the current year is: 1943\",\n",
    "    \"The YYYY of the current year is: 1958\",\n",
    "    \"The YYYY of the current year is: 1970\",\n",
    "    \"The YYYY of the current year is: 1982\",\n",
    "    \"The YYYY of the current year is: 1991\",\n",
    "    \"The YYYY of the current year is: 2001\",\n",
    "    \"The YYYY of the current year is: 2009\",\n",
    "    \"The YYYY of the current year is: 2014\",\n",
    "    \"The YYYY of the current year is: 1913\",\n",
    "    \"The YYYY of the current year is: 1929\",\n",
    "    \"The YYYY of the current year is: 1945\",\n",
    "    \"The YYYY of the current year is: 1952\",\n",
    "    \"The YYYY of the current year is: 1967\",\n",
    "    \"The YYYY of the current year is: 1973\",\n",
    "    \"The YYYY of the current year is: 1986\",\n",
    "    \"The YYYY of the current year is: 1992\",\n",
    "    \"The YYYY of the current year is: 2005\",\n",
    "    \"The YYYY of the current year is: 2012\",\n",
    "    \"The YYYY of the current year is: 1905\",\n",
    "    \"The YYYY of the current year is: 1920\",\n",
    "    \"The YYYY of the current year is: 1938\",\n",
    "    \"The YYYY of the current year is: 1954\",\n",
    "    \"The YYYY of the current year is: 1969\",\n",
    "    \"The YYYY of the current year is: 1981\",\n",
    "    \"The YYYY of the current year is: 1994\",\n",
    "    \"The YYYY of the current year is: 2000\",\n",
    "    \"The YYYY of the current year is: 2007\",\n",
    "    \"The YYYY of the current year is: 2013\"\n",
    "]\n",
    "\n",
    "\n",
    "GOODDATE = [\n",
    "    \"The YYYY of the current year is: 2023\",\n",
    "    \"The YYYY of the current year is: 2037\",\n",
    "    \"The YYYY of the current year is: 2052\",\n",
    "    \"The YYYY of the current year is: 2068\",\n",
    "    \"The YYYY of the current year is: 2081\",\n",
    "    \"The YYYY of the current year is: 2095\",\n",
    "    \"The YYYY of the current year is: 2016\",\n",
    "    \"The YYYY of the current year is: 2029\",\n",
    "    \"The YYYY of the current year is: 2044\",\n",
    "    \"The YYYY of the current year is: 2059\",\n",
    "    \"The YYYY of the current year is: 2073\",\n",
    "    \"The YYYY of the current year is: 2088\",\n",
    "    \"The YYYY of the current year is: 2100\",\n",
    "    \"The YYYY of the current year is: 2019\",\n",
    "    \"The YYYY of the current year is: 2032\",\n",
    "    \"The YYYY of the current year is: 2047\",\n",
    "    \"The YYYY of the current year is: 2061\",\n",
    "    \"The YYYY of the current year is: 2076\",\n",
    "    \"The YYYY of the current year is: 2091\",\n",
    "    \"The YYYY of the current year is: 2015\",\n",
    "    \"The YYYY of the current year is: 2028\",\n",
    "    \"The YYYY of the current year is: 2042\",\n",
    "    \"The YYYY of the current year is: 2057\",\n",
    "    \"The YYYY of the current year is: 2070\",\n",
    "    \"The YYYY of the current year is: 2085\",\n",
    "    \"The YYYY of the current year is: 2098\",\n",
    "    \"The YYYY of the current year is: 2021\",\n",
    "    \"The YYYY of the current year is: 2035\",\n",
    "    \"The YYYY of the current year is: 2049\",\n",
    "    \"The YYYY of the current year is: 2064\",\n",
    "    \"The YYYY of the current year is: 2078\",\n",
    "    \"The YYYY of the current year is: 2093\",\n",
    "    \"The YYYY of the current year is: 2017\",\n",
    "    \"The YYYY of the current year is: 2031\",\n",
    "    \"The YYYY of the current year is: 2045\",\n",
    "    \"The YYYY of the current year is: 2060\",\n",
    "    \"The YYYY of the current year is: 2074\",\n",
    "    \"The YYYY of the current year is: 2089\",\n",
    "    \"The YYYY of the current year is: 2024\",\n",
    "    \"The YYYY of the current year is: 2038\",\n",
    "    \"The YYYY of the current year is: 2053\",\n",
    "    \"The YYYY of the current year is: 2067\",\n",
    "    \"The YYYY of the current year is: 2082\",\n",
    "    \"The YYYY of the current year is: 2096\",\n",
    "    \"The YYYY of the current year is: 2020\",\n",
    "    \"The YYYY of the current year is: 2033\",\n",
    "    \"The YYYY of the current year is: 2048\",\n",
    "    \"The YYYY of the current year is: 2062\",\n",
    "    \"The YYYY of the current year is: 2077\",\n",
    "    \"The YYYY of the current year is: 2092\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (text, True) for text in GOOD\n",
    "] + [\n",
    "    (text, False) for text in BAD\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vectors' in globals():\n",
    "    del vectors\n",
    "\n",
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(0, 32),\n",
    "    save_to=None              \n",
    ")\n",
    "\n",
    "print(vectors[16])\n",
    "print(f\"Shape of vector: {vectors[16].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(16, 17):\n",
    "    for multiplier in range(-10, 11):\n",
    "        print(f\"Layer: {layer}, Multiplier: {multiplier}\")\n",
    "        result = run_simple_steering(\n",
    "            text=[\"The YYYY of the current year is:\"],\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            layer=layer,\n",
    "            multiplier=multiplier,\n",
    "            vector=vectors[15],\n",
    "            max_n_tokens=13,\n",
    "            save_to=None,\n",
    "        )\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for multiplier in range(-10, 11):\n",
    "    result = run_simple_steering(\n",
    "        text=[\"I think that this cat is\"],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        layer=6,\n",
    "        multiplier=multiplier,\n",
    "        vector=vectors[16],\n",
    "        max_n_tokens=20,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    # Ensure both tensors are on the same device (CPU) and are float type\n",
    "    v1 = v1.cpu().detach().float()\n",
    "    v2 = v2.cpu().detach().float()\n",
    "    \n",
    "    # Ensure the shape is (1, 4096)\n",
    "    v1 = v1.view(1, 4096)\n",
    "    v2 = v2.view(1, 4096)\n",
    "    \n",
    "    # Calculate cosine similarity using PyTorch operations\n",
    "    similarity = torch.nn.functional.cosine_similarity(v1, v2, dim=1)\n",
    "    \n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(probe.weight, vectors[16])\n",
    "print(f\"Cosine similarity between probe.weight and vectors[16]: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
